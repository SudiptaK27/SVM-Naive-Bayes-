{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is a Support Vector Machine (SVM), and how does it work?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for both classification and regression, but it is most widely applied to classification problems. The main goal of an SVM is to find the best possible boundary that separates classes in the feature space. This boundary is known as a hyperplane.\n",
        "\n",
        "**How SVM Works**\n",
        "\n",
        "\n",
        "**1. Finding the Optimal Hyperplane**\n",
        "\n",
        "In a classification task, an SVM tries to find a hyperplane that best separates the data into different classes.\n",
        "A “best” hyperplane is one that:\n",
        "\n",
        "maximizes the margin (the distance between the hyperplane and the nearest data points), and\n",
        "\n",
        "correctly classifies the data points as much as possible.\n",
        "\n",
        "The data points that lie closest to the hyperplane are called support vectors, and they determine the position and orientation of the hyperplane.\n",
        "\n",
        "**2. Maximizing the Margin**\n",
        "\n",
        "The margin is the space between the separating hyperplane and the closest data points from each class.\n",
        "A larger margin means:\n",
        "\n",
        "better generalization,\n",
        "\n",
        "more stable predictions, and\n",
        "\n",
        "reduced risk of overfitting.\n",
        "\n",
        "SVM chooses the hyperplane that gives the maximum margin, making the classifier robust.\n",
        "\n",
        "**3. Handling Non-Linearly Separable Data (Kernel Trick)**\n",
        "\n",
        "In many real problems, the data is not linearly separable. SVM uses a concept called the kernel trick, which transforms the data into a higher-dimensional space where a linear separator can be found.\n",
        "\n",
        "**Common kernels:**\n",
        "\n",
        "Linear Kernel – for linearly separable data\n",
        "\n",
        "Polynomial Kernel – captures curved boundaries\n",
        "\n",
        "RBF (Radial Basis Function) Kernel – handles complex, non-linear relationships\n",
        "\n",
        "Sigmoid Kernel – similar to neural networks\n",
        "\n",
        "The kernel trick allows SVM to build powerful non-linear classifiers without explicitly computing the high-dimensional transformation.\n",
        "\n",
        "**4. Soft Margin and Regularization**\n",
        "\n",
        "Real-world datasets often contain noise or overlapping classes. To manage this, SVM introduces a soft margin, allowing certain misclassifications while still aiming for a large margin.\n",
        "\n",
        "A parameter called C controls this:\n",
        "\n",
        "High C → less tolerance for misclassification (risk of overfitting).\n",
        "\n",
        "Low C → more tolerance (risk of underfitting).\n",
        "\n",
        "**Summary**\n",
        "\n",
        "**SVM works by:**\n",
        "\n",
        "Mapping data into a feature space.\n",
        "\n",
        "Finding an optimal hyperplane that separates classes with maximum margin.\n",
        "\n",
        "Using support vectors (critical boundary points) to define the decision boundary.\n",
        "\n",
        "Applying kernels to handle non-linear patterns.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Support Vector Machines are powerful, flexible, and effective for high-dimensional and complex datasets. By maximizing the margin and using kernel functions, SVMs achieve strong classification performance and are widely used in areas such as image recognition, text classification, bioinformatics, and medical diagnosis.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Question 2: Explain the difference between Hard Margin and Soft Margin SVM.**\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "Support Vector Machines (SVMs) aim to find a hyperplane that separates classes with the maximum possible margin. The concepts of Hard Margin and Soft Margin represent two different approaches to controlling this separation depending on whether the dataset is perfectly separable or contains noise and overlaps.\n",
        "\n",
        "**Hard Margin SVM**\n",
        "\n",
        "A Hard Margin SVM assumes that the data is perfectly linearly separable, meaning a clear straight boundary exists with zero classification errors.\n",
        "The algorithm tries to find a hyperplane such that:\n",
        "\n",
        "All training points are correctly classified.\n",
        "\n",
        "No point lies within the margin.\n",
        "\n",
        "The margin is maximized.\n",
        "\n",
        "Characteristics of Hard Margin SVM\n",
        "\n",
        "Works only when there is no noise, no outliers, and clear separation.\n",
        "\n",
        "All points must stay outside the margin boundaries.\n",
        "\n",
        "Any violation of the margin is not allowed.\n",
        "\n",
        "Very sensitive to noise—one outlier can completely distort the hyperplane.\n",
        "\n",
        "**When Hard Margin Is Used**\n",
        "\n",
        "Ideal for clean, well-separated datasets where perfect classification is possible.\n",
        "\n",
        "**Soft Margin SVM**\n",
        "\n",
        "A Soft Margin SVM allows the hyperplane to make some mistakes by permitting certain points to lie inside the margin or even be misclassified.\n",
        "This is controlled using a regularization parameter C, which balances margin width and classification errors.\n",
        "\n",
        "**Characteristics of Soft Margin SVM**\n",
        "\n",
        "Allows misclassification of difficult or noisy points.\n",
        "\n",
        "Introduces slack variables to measure how much each point violates the margin.\n",
        "\n",
        "Improves generalization on real-world data.\n",
        "\n",
        "More robust to noise and outliers.\n",
        "\n",
        "**Role of C (Regularization Parameter)**\n",
        "\n",
        "High C: Model tries to classify every point correctly → smaller margin → risk of overfitting.\n",
        "\n",
        "Low C: Model allows more margin violations → wider margin → better generalization.\n",
        "\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Hard Margin SVM is strict and suitable only for datasets that are perfectly separable without noise, while Soft Margin SVM is flexible and better suited for real-world problems where overlap and outliers are unavoidable. By allowing controlled violations of the margin, Soft Margin SVM achieves better generalization and is widely used in modern machine learning applications.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.**\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "The Kernel Trick is a fundamental technique used in Support Vector Machines (SVM) to handle datasets that are not linearly separable in their original feature space. Instead of trying to draw a straight hyperplane in the existing space, the kernel trick allows the SVM to implicitly map the data into a higher-dimensional space where a linear separation becomes possible.\n",
        "\n",
        "The key idea is that this transformation to higher dimensions is never computed explicitly. Instead, the kernel trick uses a kernel function to compute the inner product between the transformed feature vectors directly. This makes the computation efficient even in extremely high-dimensional spaces.\n",
        "\n",
        "**Why the Kernel Trick Is Useful**\n",
        "\n",
        "Many real-world datasets have complex, non-linear relationships.\n",
        "\n",
        "A simple linear boundary cannot separate the classes.\n",
        "\n",
        "Mapping the data to a higher-dimensional space reveals patterns that were not visible before.\n",
        "\n",
        "The kernel trick makes this process computationally feasible and fast.\n",
        "\n",
        "Example of a Kernel: RBF (Radial Basis Function) Kernel\n",
        "Kernel Function:\n",
        "\n",
        "RBF computes similarity between two points based on their distance.\n",
        "\n",
        "**Use Case:**\n",
        "\n",
        "The RBF kernel is used when:\n",
        "\n",
        "The data is non-linear and forms curved boundaries.\n",
        "\n",
        "Classes cannot be separated using simple straight lines.\n",
        "\n",
        "There are clusters or circular/elliptical decision regions.\n",
        "\n",
        "You need a flexible model that can adapt to complex patterns.\n",
        "\n",
        "**Real-world examples:**\n",
        "\n",
        "Image classification\n",
        "\n",
        "Medical diagnosis\n",
        "\n",
        "Speech recognition\n",
        "\n",
        "Any problem where decision boundaries are irregular\n",
        "\n",
        "The RBF kernel is popular because it can create highly complex decision boundaries, making it ideal for challenging classification tasks.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "The kernel trick enables SVMs to classify complex, non-linear data by implicitly transforming it into a higher-dimensional space. The RBF kernel, one of the most widely used kernels, helps SVMs model curved and intricate decision boundaries, making the algorithm powerful and adaptable for many real-world applications.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?**\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "A Naïve Bayes Classifier is a probabilistic machine learning model used primarily for classification tasks. It is based on Bayes’ Theorem, which describes the probability of a class given a set of features. Naïve Bayes models are widely used in applications such as spam detection, sentiment analysis, document classification, medical diagnosis, and recommendation systems.\n",
        "\n",
        "**What Is a Naïve Bayes Classifier?**\n",
        "\n",
        "A Naïve Bayes classifier predicts the class of a given input by calculating the posterior probability of each class based on the likelihood of the features. It chooses the class with the highest posterior probability.\n",
        "\n",
        "The prediction is based on Bayes’ Theorem, which is expressed as:\n",
        "\n",
        "P(Class | Features) = (P(Features | Class) × P(Class)) / P(Features)\n",
        "\n",
        "This means the model uses:\n",
        "\n",
        "Prior Probability\n",
        "\n",
        "The overall probability of a class occurring.\n",
        "\n",
        "Likelihood\n",
        "\n",
        "The probability of observing the features given the class.\n",
        "\n",
        "Evidence\n",
        "\n",
        "The probability of observing those features in general.\n",
        "\n",
        "Naïve Bayes works especially well when the dataset is large, high-dimensional, or consists of text data.\n",
        "\n",
        "**Why Is It Called “Naïve”?**\n",
        "\n",
        "The classifier is called “naïve” because it assumes that all features are conditionally independent of each other, given the class label.\n",
        "This assumption is rarely true in real-world datasets.\n",
        "\n",
        "**Why the Independence Assumption Is Naïve:**\n",
        "\n",
        "In real life, features often influence each other.\n",
        "Example: In a medical dataset, blood pressure and cholesterol levels are usually correlated.\n",
        "\n",
        "Naïve Bayes ignores these relationships and treats features as if they are unrelated.\n",
        "\n",
        "Despite this unrealistic assumption, the classifier still performs surprisingly well in many practical applications.\n",
        "\n",
        "**Why Naïve Bayes Works Well Despite Being Naïve**\n",
        "\n",
        "**Simplicity and Speed:**\n",
        "Calculation is fast because only conditional probabilities are required.\n",
        "\n",
        "**Works with High-Dimensional Data:**\n",
        "Especially effective in text classification where thousands of features (words) exist.\n",
        "\n",
        "**Performs Well with Limited Training Data:**\n",
        "Needs relatively little data to estimate probabilities.\n",
        "\n",
        "**Robust to Irrelevant Features:**\n",
        "Even if some features are not useful, Naïve Bayes handles them gracefully.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "A Naïve Bayes Classifier is a simple yet powerful probabilistic model built on Bayes’ Theorem. It is called “naïve” because it assumes that all input features are independent of each other, an assumption that rarely holds true. Nevertheless, its efficiency, accuracy in many domains, and ability to handle high-dimensional data make it a popular choice for classification tasks.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?**\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "Naïve Bayes Classifiers come in different variants, each designed for a specific type of data. The three most commonly used types are Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes. All three use Bayes’ Theorem but differ in the way they model feature distributions. Choosing the correct variant depends on the characteristics of the dataset.\n",
        "\n",
        "**1. Gaussian Naïve Bayes**\n",
        "\n",
        "**Description**\n",
        "\n",
        "Gaussian Naïve Bayes assumes that continuous numerical features follow a normal (Gaussian) distribution.\n",
        "It calculates the likelihood of each feature value using the mean and variance of the feature within each class.\n",
        "\n",
        "This variant is suitable when the features are real-valued and the distribution of data points resembles a bell curve.\n",
        "\n",
        "**When to Use Gaussian NB**\n",
        "\n",
        "Continuous features such as height, weight, test scores, temperature, blood pressure, or any sensor readings.\n",
        "\n",
        "Medical datasets with continuous lab measurements.\n",
        "\n",
        "Any dataset where features follow (or approximately follow) a normal distribution.\n",
        "\n",
        "Example: Predicting whether a tumor is benign or malignant based on continuous diagnostic measurements.\n",
        "\n",
        "**2. Multinomial Naïve Bayes**\n",
        "\n",
        "**Description**\n",
        "\n",
        "Multinomial Naïve Bayes is designed for discrete count data.\n",
        "It works well when features represent the number of times an event occurs.\n",
        "\n",
        "Typical feature examples include word counts or token frequencies in documents.\n",
        "\n",
        "When to Use Multinomial NB\n",
        "\n",
        "Text classification tasks (spam detection, document categorization).\n",
        "\n",
        "Bag-of-Words (BoW) or TF-IDF representations of documents.\n",
        "\n",
        "Problems involving count features, such as the number of clicks, purchases, or visits.\n",
        "\n",
        "Example: Classifying emails into spam or not spam, based on word frequency counts.\n",
        "\n",
        "**3. Bernoulli Naïve Bayes**\n",
        "\n",
        "\n",
        "**Description**\n",
        "\n",
        "Bernoulli Naïve Bayes works with binary or boolean features.\n",
        "Instead of counts, it considers whether a feature is present or absent.\n",
        "\n",
        "It assumes each feature is either 0 or 1, such as whether a word appears in a document.\n",
        "\n",
        "When to Use Bernoulli NB\n",
        "\n",
        "Binary text data (e.g., “word present/not present”).\n",
        "\n",
        "Feature sets containing yes/no, true/false, or 0/1 values.\n",
        "\n",
        "Situations where frequency of features is not important—only presence matters.\n",
        "\n",
        "Example: Sentiment analysis using binary features to indicate if specific keywords appear in a review.\n",
        "\n",
        "\n",
        "\n",
        "The three Naïve Bayes variants—Gaussian, Multinomial, and Bernoulli—are specialized for different types of data. Gaussian handles continuous features, Multinomial works with count-based features, and Bernoulli is best for binary indicators. Understanding these differences ensures the correct model is selected, leading to higher accuracy and better performance in classification tasks.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p6BbATsm_QWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qvtWS0lrJwk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train an SVM classifier with a linear kernel\n",
        "model = SVC(kernel=\"linear\")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict and compute accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 5. Print results\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"\\nNumber of Support Vectors for each class:\", model.n_support_)\n",
        "print(\"\\nSupport Vectors:\\n\", model.support_vectors_)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "dk3ZHAfQGwGY",
        "outputId": "723ee3b2-0319-452f-e6e8-a299b5c3a546"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Number of Support Vectors for each class: [ 3 11 11]\n",
            "\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nSample Output (values may differ):\\n\\nModel Accuracy: 1.0\\nNumber of Support Vectors for each class: [3 3 3]\\nSupport Vectors:\\n [[5.1 3.5 1.4 0.2]\\n  [5.4 3.9 1.7 0.4]\\n  [5.   3.4 1.5 0.2]\\n  ...\\n]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HMGqJShiJ0zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "'''\n",
        "\n",
        "# Question 7:\n",
        "# Load the Breast Cancer dataset\n",
        "# Train a Gaussian Naïve Bayes model\n",
        "# Print its classification report including precision, recall, and F1-score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Gaussian Naive Bayes\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKE8IsxVG6VA",
        "outputId": "265ae3a5-f497-41ea-80b4-87526110a2ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tmqPvV1IJ311"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy.\n",
        "'''\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define the parameter grid\n",
        "param_grid = {\n",
        "    \"C\": [0.1, 1, 10, 100],\n",
        "    \"gamma\": [\"scale\", \"auto\", 0.01, 0.001],\n",
        "    \"kernel\": [\"rbf\"]\n",
        "}\n",
        "\n",
        "# 4. GridSearchCV setup\n",
        "grid = GridSearchCV(\n",
        "    estimator=SVC(),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "\n",
        "# 5. Fit the grid search\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 6. Best model and predictions\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# 7. Print results\n",
        "print(\"Best Hyperparameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juoac66lHOLh",
        "outputId": "dc11acf2-0243-44ac-e6e0-b550c274900a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "Accuracy: 0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "00npBXGAJ6Wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load a subset of the 20 Newsgroups dataset (to keep it simple)\n",
        "categories = [\"sci.space\", \"rec.sport.baseball\", \"comp.graphics\"]\n",
        "data = fetch_20newsgroups(subset=\"all\", categories=categories)\n",
        "\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Vectorize text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "X_vec = vectorizer.fit_transform(X)\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_vec, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train a Multinomial Naive Bayes classifier\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict probabilities\n",
        "y_prob = model.predict_proba(X_test)\n",
        "\n",
        "# 6. Compute ROC-AUC (multiclass using One-vs-Rest)\n",
        "y_test_binarized = label_binarize(y_test, classes=np.unique(y))\n",
        "\n",
        "roc_auc = roc_auc_score(y_test_binarized, y_prob, multi_class=\"ovr\")\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtzoQd5iHhIy",
        "outputId": "320da1ce-cd9c-44f9-d39a-abaa3c4dd88e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.999530117082478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "u9UsuC9VJ81b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "(Include your Python code and output in the code box below.)**\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "Email classification is a classic machine-learning problem where the goal is to correctly label emails as Spam or Not Spam. The dataset usually contains raw text, incomplete information, and highly imbalanced class distribution. A systematic approach is required to build a reliable and business-ready solution.\n",
        "\n",
        "**1. Data Preprocessing**\n",
        "\n",
        "\n",
        "**a) Handling Missing Data**\n",
        "\n",
        "Email datasets may contain missing subjects, empty bodies, or incomplete metadata.\n",
        "\n",
        "**Approach:**\n",
        "\n",
        "Replace missing text fields with an empty string (\"\") rather than dropping rows.\n",
        "\n",
        "For metadata features (e.g., sender info, timestamp), use mode imputation or leave them out if not helpful.\n",
        "\n",
        "This ensures that no important training example is lost.\n",
        "\n",
        "**b) Text Preprocessing and Vectorization**\n",
        "\n",
        "Emails contain unstructured text with diverse vocabulary. Machine learning models cannot work directly with raw text, so it must be converted into numeric form.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "Convert text to lowercase\n",
        "\n",
        "Remove punctuation, numbers, special symbols\n",
        "\n",
        "Remove stopwords (words like the, is, and)\n",
        "\n",
        "Apply TF-IDF vectorization\n",
        "\n",
        "Captures how important a word is in an email\n",
        "\n",
        "Reduces weight of commonly occurring words\n",
        "\n",
        "Works extremely well for spam detection\n",
        "\n",
        "TF-IDF transforms the email text into a meaningful numerical representation suitable for machine learning.\n",
        "\n",
        "**2. Choosing an Appropriate Model (SVM vs. Naïve Bayes)**\n",
        "\n",
        "**Option 1: Naïve Bayes (Multinomial/Bernoulli)**\n",
        "\n",
        "Fast to train\n",
        "\n",
        "Excellent for high-dimensional text\n",
        "\n",
        "Performs very well with word-frequency features\n",
        "\n",
        "Robust with sparse data\n",
        "\n",
        "**Option 2: Support Vector Machine (SVM)**\n",
        "\n",
        "Works extremely well for text classification\n",
        "\n",
        "Handles high-dimensional TF-IDF vectors\n",
        "\n",
        "Excellent margin-based classifier\n",
        "\n",
        "Produces very strong accuracy\n",
        "\n",
        "**Justification**\n",
        "\n",
        "For spam classification, both models perform well, but:\n",
        "\n",
        "SVM often gives higher accuracy and better separation.\n",
        "\n",
        "Naïve Bayes is extremely fast and performs surprisingly well with text.\n",
        "\n",
        "**Recommended Choice:**\n",
        "Start with Naïve Bayes because it is simple, fast, and works well for spam detection.\n",
        "Then evaluate SVM as a second, stronger model and compare results.\n",
        "\n",
        "**3. Handling Class Imbalance**\n",
        "\n",
        "Email datasets usually contain far more legitimate emails than spam.\n",
        "\n",
        "**Solutions:**\n",
        "\n",
        "**a) Class Weight Adjustment**\n",
        "\n",
        "Give more weight to the minority class (spam):\n",
        "SVM(class_weight=\"balanced\")\n",
        "\n",
        "**b) Oversampling the Minority Class**\n",
        "\n",
        "SMOTE (Synthetic Minority Oversampling Technique) for numeric features\n",
        "\n",
        "Or simple random oversampling\n",
        "\n",
        "**c) Adjust Probability Threshold**\n",
        "\n",
        "If recall for spam is low, reduce classification threshold (e.g., 0.5 → 0.3).\n",
        "\n",
        "**4. Performance Evaluation**\n",
        "\n",
        "Spam classification errors have different business impacts. Metrics must reflect this.\n",
        "\n",
        "**Key Metrics:**\n",
        "\n",
        "a) Accuracy – overall correctness (not enough for imbalanced data).\n",
        "b) Precision (Spam class) – of all emails predicted as spam, how many were truly spam?\n",
        "c) Recall (Spam class) – how many spam emails were correctly detected?\n",
        "\n",
        "Missing spam emails (“false negatives”) risks security and phishing.\n",
        "d) F1-Score – balance between precision and recall.\n",
        "e) ROC-AUC – overall classifier ability.\n",
        "\n",
        "Focus Metric:\n",
        "Recall for Spam class, because missing harmful emails is more dangerous than misclassifying a few legitimate ones.\n",
        "\n",
        "**5. Business Impact of the Solution**\n",
        "\n",
        "**A reliable spam classifier provides major benefits:**\n",
        "\n",
        "Improved Security\n",
        "\n",
        "Blocks phishing, fraud, and malware emails\n",
        "\n",
        "Protects company systems and employees\n",
        "\n",
        "Higher Productivity\n",
        "\n",
        "Reduces time wasted sorting through spam\n",
        "\n",
        "Cost Savings\n",
        "\n",
        "Lowers IT support costs related to email threats\n",
        "\n",
        "Brand Protection\n",
        "\n",
        "Stops malicious emails sent from compromised accounts\n",
        "\n",
        "Better User Experience\n",
        "\n",
        "Keeps inboxes clean and reduces user frustration\n",
        "\n",
        "Overall, the model enhances cybersecurity, efficiency, and operational performance."
      ],
      "metadata": {
        "id": "2ZNUv3RiH-9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# 1. Load text dataset (simulated spam vs ham categories)\n",
        "categories = [\"comp.windows.x\", \"rec.sport.hockey\"]  # treat as ham vs spam\n",
        "data = fetch_20newsgroups(subset=\"all\", categories=categories)\n",
        "\n",
        "X = data.data\n",
        "y = data.target   # 0 or 1\n",
        "\n",
        "# 2. Handle missing text\n",
        "X = [\" \" if text is None else text for text in X]\n",
        "\n",
        "# 3. TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "X_vec = vectorizer.fit_transform(X)\n",
        "\n",
        "# 4. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_vec, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 5. Train Naïve Bayes Classifier\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_prob = model.predict_proba(X_test)\n",
        "\n",
        "# 7. ROC-AUC score (binary case)\n",
        "roc_auc = roc_auc_score(y_test, y_prob[:, 1])\n",
        "\n",
        "# 8. Print Results\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vMzK9OeIXpV",
        "outputId": "4ffd3009-4041-4c7a-d73a-23b93dab611a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       189\n",
            "           1       0.99      1.00      0.99       209\n",
            "\n",
            "    accuracy                           0.99       398\n",
            "   macro avg       0.99      0.99      0.99       398\n",
            "weighted avg       0.99      0.99      0.99       398\n",
            "\n",
            "ROC-AUC Score: 0.999797473481684\n"
          ]
        }
      ]
    }
  ]
}